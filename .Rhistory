1:10
seq(1,10)
seq(1,10,2)
a <- seq(1,10,2)
str(a)
b <- (10,20,30)
b <- c(10,20,30)
?c
10:-2:1
list(name="test", age=45)
c <- list(name="test", age=45)
d = list(name="test", age=45)
doe = list(name="test", age=23)
doe[0]
doe[1]
doe[2]
doe[[2]]
str(doe[2])
str(doe[[2]])
doe[2:3]
View
?View
View(iris)
iris["Species"]
levels(iris["Species"])
levels(iris$Species)
levels(Species)
levels(iris["Species"])
levels(iris$Species)
iris[1,]
iris[,]
library(ggplot2)
library(ggplot)
package(ggplot2)
install.packages(ggplot2)
install.packages("ggplot2"")
install.packages("ggplot2")
library(ggplot2) # load the ggplot library
s
library(ggplot2) # load the ggplot library
d
library(ggplot2)
t <- matrix(1, 2, 3, 4)
colnames(t) <- c(x, y)
ccolnames(t) <- c("x", "y")
t <- matrix(1, 2, 3, 4, ncol=2, byrow=TRUE)
t <- matrix(c(1, 2, 3, 4), ncol=2, byrow=TRUE)
colnames(t) <- c("x", "y")
myplot <- ggplot(data=t) + geom_point(mapping=aes(x=x,y=y)) # can store plots in a variable
myplot <- ggplot(data=t) + geom_point(mapping=aes(x=x,y=y)) # can store plots in a variable
myplot <- ggplot(data=t) + geom_point(mapping=aes(x=x,y=y))
t <- data.frame("x" = c(1, 2), "y" = c(3,4))
myplot <- ggplot(data=t) + geom_point(mapping=aes(x=x,y=y))
myplot
x <-c(	45,	36,	37,	43,	38,	49,	39,	43,	44,	38,	42,	40)
x
y <-c(	43,	35,	34,	41,	44,	44,	42,	46,	39,	39,	47,	39)
t <- data.frame("x" = x, "y" = y)
myplot <- ggplot(data=t) + geom_point(mapping=aes(x=x,y=y)) # can store plots in a variable
myplot
q(0)
q()
q
library(ggplot2) # load the ggplot library
x <-c(	45,	36,	37,	43,	38,	49,	39,	43,	44,	38,	42,	40)
y <-c(	43,	35,	34,	41,	44,	44,	42,	46,	39,	39,	47,	39)
x
y
t <- data.frame("x" = x, "y" = y)
fun.1 <- function(x) 1688.840 + 0.741 * x
myplot <- ggplot(data=t) + geom_point(mapping=aes(x=x,y=y)) + stat_function(fun = fun.1)
myplot
fun.1 <- function(x) 15.426 + 0.599 * x
myplot
myplot <- ggplot(data=t) + geom_point(mapping=aes(x=x,y=y)) + stat_function(fun = fun.1)
myplot
q(0)
q()
q
ii.	survey <- read.csv("survey2019.csv")
survey <- read.csv("survey2019.csv")
setwd("C:/Users/kd/Google Drive/School/CSUF_Class/CPSC_375_data_science/hw1")
setwd("C:/Users/kd/Google Drive/School/CSUF_Class/CPSC_375_data_science/hw1")
i.	setwd("C:/Users/kd/Google Drive/School/CSUF_Class/CPSC_375_data_science/hw1")
setwd("C:/Users/kd/Google Drive/School/CSUF_Class/CPSC_375_data_science/hw1")
q
setwd("/Users/mikedo/Google Drive/School/CSUF_Class/CPSC_375_data_science/hw1")
library(ggplot2)
#install.packages("tidyverse")
library(tidyverse)
setwd("/Users/mikedo/Google Drive/School/CSUF_Class/CPSC_375_data_science/hw1")
survey <- read.csv("survey2019.csv")
seq <- 1:10
mathList = survey %>% group_by(Math) %>% summarise(Count=n()) %>% mutate(Item=Math, Discipline="Math") %>% select(Item, Count, Discipline) %>% arrange(Item)
#mathList <- mathList %>% complete(Item = seq, fill = list(Count = 0, Discipline="Math"))
csList = survey %>% group_by(CS) %>% summarise(Count=n()) %>% mutate(Item=CS, Discipline="CS") %>% select(Item, Count, Discipline) %>% arrange(Item)
#csList <- csList %>% complete(Item = seq, fill = list(Count = 0, Discipline="CS"))
#totalList <- merge(mathList, csList, by="Item")
totalList <- bind_rows(mathList, csList)
#scatter plot
p <- ggplot(data = totalList)
graph =
p +
geom_point(mapping=aes(x=totalList$Item, y=totalList$Count, colour=totalList$Discipline)) +
ggtitle("Q1 Survey of Students With Their Levels of Math and CS") +
theme(plot.title = element_text(hjust = 0.5)) +
xlab("Skill Levels") +
ylab("# of Students") +
scale_x_discrete(breaks=totalList$Item, labels=as.character(totalList$Item), limits=totalList$Item) +
theme(legend.position = c(0.95, 0.95), legend.justification = c("right", "top")) +
labs(fill = "Dose (mg)")
pdf("question2d_graph.pdf")
print(graph)
dev.off()
str(iris)
sample(1:10, 1) # function to return a random number
sample(1:100, 1)
sample(1:100, 5)
trainindex = sample(1:150, 100) # don't pick 1:100
trainfeatures = iris[trainindex,1:4]
View(trainfeatures)
View(iris)
trainlabels = iris[trainindex, "Species"]
str(trainlabels)
trainlabels[1]
trainlabels[0]
trainlabels[-1]
trainlabels[0]
x <- [1, 2, 3]
x <- c(1, 2, 3)
x[0]
x[1]
trainlabels[1]
trainlabels[0]
trainlabels[1]
trainlabels[2]
trainlabels
trainlabels[3]
trainlabels[4]
predictedlabels <- knn(trainfeatures=trainfeatures, cl=trainlabels, test=testfeatures, k=1)
library(class)
predictedlabels <- knn(trainfeatures=trainfeatures, cl=trainlabels, test=testfeatures, k=1)
predictedlabels <- knn(trainfeatures=trainfeatures, cl=trainlabels, test=testfeatures, k=1)
trainfeatures = iris[trainindex,1:4]
predictedlabels <- knn(trainfeatures=trainfeatures, cl=trainlabels, test=testfeatures, k=1)
predictedlabels <- knn(train=trainfeatures, cl=trainlabels, test=testfeatures, k=1)
trainlabels = iris[trainindex, "Species"]
testindex = setdiff(1:150, trainindex) #101:150
testfeatures = iris[testindex, 1:4]
predictedlabels <- knn(train=trainfeatures, cl=trainlabels, test=testfeatures)
predictedlabels
predictedlabels <- knn(train=trainfeatures, cl=trainlabels, test=testfeatures, k=1)
library(tidyverse)
# visualize the variables to model
ggplot(data=iris) + geom_point(mapping=aes(x=Sepal.Width, y=Sepal.Length))
# linear model
lm(data=iris, Sepal.Length~Sepal.Width)
# save model to a variable
mod <- lm(data=iris, Sepal.Length~Sepal.Width)
# print the model parameters and evaluation metric
summary(mod)
# model parameters
mod$coefficients
mod$coefficients[1] # slope
mod$coefficients[2] # intercept
# plot best fit line over the scatterplot
ggplot(data=iris) + geom_point(mapping=aes(x=Sepal.Width, y=Sepal.Length)) + geom_abline(slope=mod$coefficients[2], intercept = mod$coefficients[1])
# model another pair of variables
mod <- lm(data=iris, Petal.Length~Petal.Width)
ggplot(data=iris) + geom_point(mapping=aes(x=Petal.Width, y=Petal.Length)) + geom_abline(slope=mod$coefficients[2], intercept = mod$coefficients[1])
summary(mod)
# add residuals to the dataset
library(modelr)
irisCopy <- iris %>% add_residuals(mod)
View(irisCopy)
ggplot(data=irisCopy) + geom_histogram(mapping = aes(x=resid))
ggplot(data=irisCopy) + geom_histogram(mapping=aes(x=resid))
mod <- lm(data=iris, Petal.Length~Petal.Width + Sepal.Width)
summary(mod)
# linear modeling with two independent variables, Species is categorical (factor)
mod <- lm(data=iris, Petal.Length~Petal.Width + Species)
summary(mod)
ggplot(data=iris) + geom_point(mapping=aes(x=Petal.Width, y=Petal.Length)) + geom_abline(slope=c[2], intercept = c[1])
ggplot(data=iris) + geom_point(mapping=aes(x=Petal.Width, y=Petal.Length)) + geom_abline(slope=c[2], intercept = c[1]) + geom_abline(slope = c[2], intercept = c[1]+c[3])
library(modelr)
# linear modeling with two independent variables, Species is categorical (factor)
mod <- lm(data=iris, Petal.Length~Petal.Width + Species)
summary(mod)
# overlay the best fit lines for each value of Species
c <- mod$coefficients
ggplot(data=iris) + geom_point(mapping=aes(x=Petal.Width, y=Petal.Length)) + geom_abline(slope=c[2], intercept = c[1])
ggplot(data=iris) + geom_point(mapping=aes(x=Petal.Width, y=Petal.Length)) + geom_abline(slope=c[2], intercept = c[1]) + geom_abline(slope = c[2], intercept = c[1]+c[3])
ggplot(data=iris) + geom_point(mapping=aes(x=Petal.Width, y=Petal.Length)) + geom_abline(slope=c[2], intercept = c[1]) + geom_abline(slope = c[2], intercept = c[1]+c[3]) + geom_abline(slope = c[2], intercept = c[1]+c[4])
ggplot(data=iris) + geom_point(mapping=aes(x=Petal.Width, y=Petal.Length, color=Species)) + geom_abline(slope=c[2], intercept = c[1]) + geom_abline(slope = c[2], intercept = c[1]+c[3]) + geom_abline(slope = c[2], intercept = c[1]+c[4])
library(tidyverse)
?mpg
ggplot(data=mpg, mapping = aes(x=displ)) + geom_point(mapping = aes(y=cty))
mod <- lm(cty ~ displ, data=mpg)
mod <- lm(cty ~ displ, data=mpg)
summary(mod) # R2 is low: not a good model
plot of best fit line shows why: relationship is more of a decreasing exponential instead of linear
ggplot(data=mpg) + geom_point(mapping = aes(x=displ, y=cty)) + geom_abline(slope=mod$coefficients[2], intercept = mod$coefficients[1], color="blue")
mpgexp <- mutate(mpg, displ_exp = exp(-displ))
mod <- lm(cty ~ displ_exp, data=mpgexp)
summary(mod) # R2 is higher now
# goal: to predict city mileage (cty) given engine displacement (displ)
ggplot(data=mpg, mapping = aes(x=displ)) + geom_point(mapping = aes(y=cty))
# first, try linear modeling with original variable
mod <- lm(cty ~ displ, data=mpg)
summary(mod) # R2 is low: not a good model
data_grid(displ)
mygrid <- mpg %>% data_grid(displ)
View(mygrid)
# add a new transformed variable as before
mygrid <- mygrid %>% mutate(displ_exp = exp(-displ))
# add predictions from the model
mygrid <- mygrid %>% add_predictions(mod)
# add predictions from the model
mygrid <- mygrid %>% add_predictions(mod)
library(modelr) # tidyverse library with useful wrapper functions
# create a new dataset with only unique values of the independent variable (displ)
mygrid <- mpg %>% data_grid(displ)
# add a new transformed variable as before
mygrid <- mygrid %>% mutate(displ_exp = exp(-displ))
# add predictions from the model
mygrid <- mygrid %>% add_predictions(mod)
# overlay a line using the new dataset
ggplot(data=mpg, mapping = aes(x=displ)) + geom_point(mapping = aes(y=cty)) +
geom_line(data=mygrid,mapping=aes(x=displ, y=pred), color="blue")
library(ggplot2)
library(tidyverse)
parialDeriveJ <- function(){
}
# setwd("/Users/mikedo/Desktop")
# setwd("C:/Users/kdo1/Google Drive/School/CSUF_Class/CPSC_483_data_mining/class_exercise")
setwd("C:/temp/cpsc483/class_exercise")
data <- read.csv("dataSetLogisticRegression.txt", header = FALSE)
colnames(data) <- c("x", "y", "z")
xyMatrix = data.matrix(data[,1:2])
colnames(xyMatrix) <- c("x", "y")
clMatrix = data.matrix(data[,3])
colnames(clMatrix) <- c("z")
# matrix(c(c(2, 4, 3), c(1, 5, 7)), nrow=2, ncol=3, byrow=TRUE)
thetaMatrix <- matrix(c(0, 0, 0), nrow=3, ncol=1, byrow=TRUE)
p <- ggplot(data = data)
graph =
p +
geom_point(mapping=aes(x=x, y=y, color=z))
graph
View(thetaMatrix)
1:3
x <- 1:3
str(x)
p <- ggplot(mpg, aes(displ, cty)) + geom_point()
library(ggplot)
library(ggplot2)
p <- ggplot(mpg, aes(displ, cty)) + geom_point()
p
p$drv
View(mpg)
mpg$drv
g <- p + facet_grid(rows = vars(drv))
g
ggplot(data = iris) + geom_point(mapping=aes(x=Sepal.Length, y=Sepal.Width)) + facet_wrap(~Species)
ggplot(data = iris) + geom_point(mapping=aes(x=Sepal.Length, y=Sepal.Width)) + facet_wrap(~Species, scales = "free")
ggplot(data = iris) + geom_point(mapping=aes(x=Sepal.Length, y=Sepal.Width)) + facet_wrap(~Species)
#
# Clustering the iris data set using k-means
library(tidyverse)
# Use only the Petal lengths and widths (columns 3,4)
ggplot(data=iris) + geom_point(mapping = aes(x=Petal.Width, y=Petal.Length, color=Species))
# Run k-means with k=3
km <- kmeans(iris[,3:4], centers=3)
# Add a column with the cluster numbers (either 1,2,3)
iris$cluster <- km$cluster
View(iris)
# Run k-means with k=3
km <- kmeans(iris[,3:4], centers=3)
# Add a column with the cluster numbers (either 1,2,3)
iris$cluster <- km$cluster
# The cluster number should be a discrete variable/factor, not a numeric
iris$cluster <- as.factor(iris$cluster)
iris$cluster <- as.factor(iris$cluster)
ggplot(data=iris) + geom_point(mapping = aes(x=Petal.Width, y=Petal.Length, color=cluster))
ggplot(data=iris) + geom_point(mapping = aes(x=Petal.Width, y=Petal.Length, color=cluster))
View(iris)
km$centers
View(kcenters)
View(kcenters)
kcenters <- as.data.frame(km$centers)
View(kcenters)
ggplot(data=iris) + geom_point(mapping = aes(x=Petal.Width, y=Petal.Length, color=Species)) +
geom_point(data=kcenters, mapping=aes(x=Petal.Width, y=Petal.Length),
shape=3, size=5, color="red", stroke=2)
km <- kmeans(iris[,3:4], centers=3, nstart=10)
iris$cluster <- km$cluster
iris$cluster <- as.factor(iris$cluster)
ggplot(data=iris) + geom_point(mapping = aes(x=Petal.Width, y=Petal.Length, color=cluster))
km$centers
# store centers as a data frame to plot the centers
kcenters <- as.data.frame(km$centers)
View(kcenters)
View(iris)
# install.packages("tidyverse")
# install.packages("ggplot2")
# install.packages("class")
library(tidyverse)
library(ggplot2)
library(dplyr)
library(class)
setwd("c:/temp/cpsc375hw3")
# 2
# a. Load the data. Column 1 ("Code") is the anonymized subject code and will not be used here.
# Columns 2-10 are the 9 features. Column 11 is the diagnosis: [B]enign or [M]alignant.
data <- read_csv("breast-cancer-wisconsin.csv")
# 2ai. How many total cases are there in the data?: ___
nrow(data)
# 2aii. How many [B]enign cases are there in the data?: ___
nrow(data %>% filter(Class = "B"))
# 2aiii. How many [M]alignant cases are there in the data?: ___
nrow(data %>% filter(Class = "M"))
# b. Run the k-means clustering algorithm using all the rows and all the 9 features.
# Use k=2, nstart=10.
# i. What should be the value of k? k = ___ (already given)
# ii. Give R code:
km <- kmeans(data[,2:10], centers = 2, nstart = 10)
data$cluster <- km$cluster
data$cluster <- as.factor(data$cluster)
nrow(data %>% filter(data$Class = "B"))
nrow(data %>% filter(Class == "B"))
nrow(data %>% filter(Class == "M"))
View(data)
nrow(data %>% filter(cluster == 1))
nrow(data %>% filter(cluster == 1 & Class == "M" ))
nrow(data %>% filter(cluster == 1 & Class == "B"))
nrow(data %>% filter(cluster == 2 & Class == "M" ))
nrow(data %>% filter(cluster == 2 & Class == "B"))
nrow(data %>% filter(cluster == 1 & Class == "B"))
nrow(data %>% filter(cluster == 2 & Class == "M" ))
k1M <= nrow(data %>% filter(cluster == 1 & Class == "M"))
k1B <= nrow(data %>% filter(cluster == 1 & Class == "B"))
# cluster = 1, class == "M"
k2M <= nrow(data %>% filter(cluster == 2 & Class == "M"))
k2B <= nrow(data %>% filter(cluster == 2 & Class == "B"))
contingencyTable <- matrix(c(k1M, k1B), c(k2M, k2B), nrow = 2, ncol = 2)
k1M <= nrow(data %>% filter(cluster == 1 & Class == "M"))
k1B <= nrow(data %>% filter(cluster == 1 & Class == "B"))
k1M <- nrow(data %>% filter(cluster == 1 & Class == "M"))
k1B <- nrow(data %>% filter(cluster == 1 & Class == "B"))
k2M <- nrow(data %>% filter(cluster == 2 & Class == "M"))
k2B <- nrow(data %>% filter(cluster == 2 & Class == "B"))
contingencyTable <- matrix(c(k1M, k1B), c(k2M, k2B), nrow = 2, ncol = 2)
View(contingencyTable)
View(contingencyTable)
contingencyTable <- matrix(c(c(k1M, k1B), c(k2M, k2B)), nrow = 2, ncol = 2)
View(contingencyTable)
View(contingencyTable)
contingencyTable <- matrix(c(c(k1M, k1B), c(k2M, k2B)), nrow = 2, ncol = 2)
colnames(contingencyTable) <- c("M", "B")
rownames(contingencyTable) <- paste("Cluster", c(1, 2))
View(contingencyTable)
View(contingencyTable)
nrow(data)
nrow(data %>% filter(Class == "B"))
nrow(data %>% filter(Class == "M"))
# cluster = 1, class == "B"
k1M <- nrow(data %>% filter(cluster == 1 & Class == "M"))
k1B <- nrow(data %>% filter(cluster == 1 & Class == "B"))
# cluster = 1, class == "M"
k2M <- nrow(data %>% filter(cluster == 2 & Class == "M"))
k2B <- nrow(data %>% filter(cluster == 2 & Class == "B"))
contingencyTable <- matrix(c(c(k1M, k1B), c(k2M, k2B)), nrow = 2, ncol = 2)
colnames(contingencyTable) <- c("M", "B")
rownames(contingencyTable) <- paste("Cluster", c(1, 2))
View(contingencyTable)
View(contingencyTable)
km$centers
km <- kmeans(iris[,3:4], centers=3)
iris$cluster <- km$cluster
# The cluster number should be a discrete variable/factor, not a numeric
iris$cluster <- as.factor(iris$cluster)
km$centers
View(iris)
View(iris)
